<!doctype html>
<html lang="en">
	<head>
		<meta name="description" content="This my portfolio website for the course HCI, technologies">
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link href="../styles/main.css" rel="stylesheet" type="text/css"/> 
		<title>HT Portfolio</title>
	</head>
<body>
	<header>
		<nav>
			<ul>
				<li class="dropdown">
					<a href="./topics.html" class="dropbtn active">Topics</a>
					<div class="dropdown-content">
						<a href="./topics.html#LEC1">AR&amp;VR</a>
						<a href="./topics.html#LEC2">Wearables</a>
						<a href="./topics.html#LEC3">Artificial Creatures</a>
						<a href="./topics.html#LEC4">Playful Interfaces</a>
					</div>
				</li>
				<li class="dropdown">
					<a href="./workshops.html" class="dropbtn">Workshops</a>
					<div class="dropdown-content">
						<a href="./workshops.html#WS1">AR&amp;VR</a>
						<a href="./workshops.html#WS2">Arduino</a>
						<a href="./workshops.html#WS3">CV</a>
					</div>
				</li>
				<li><a href="../index.html" class="logo"><img src="../images/Logo.svg" alt=""/></a></li>
				<li class="dropdown">
					<a href="./guestlecture.html" class="dropbtn">Guest Lecture</a>
					<div class="dropdown-content">
						<a href="./guestlecture.html#GL1">Assignment</a>
						<a href="./guestlecture.html#GL2">Reflection</a>
					</div>
				</li>
				<li class="dropdown">
					<a href="./labweeks.html" class="dropbtn">Labweeks</a>
					<div class="dropdown-content">
						<a href="./labweeks.html#LW1">Product</a>
						<a href="./labweeks.html#LW2">Proces</a>
						<a href="./labweeks.html#LW3">Reflection</a>
					</div>
				</li>
			</ul>
		</nav>
	</header>
	<main class="responsiveGrid2">
	<article id="LEC1">
		<h1>AR&amp;VR</h1>
		<div>
		<h4>Healthcare</h4>
		<p>With this AR technology, doctors can now easily access a patients data. After scanning the QR Code, they will be sent to a login form. Where they can login with the credentials given by the hospital. This prevents everyone from viewing sensitive data. Then it will show the data on the QR code, they will even be able to open it as a document on their phone. This also prevents confusing the patients data, which sadly still happens.</p>
		<img src="../images/Healthcare - 1.png" alt=""/>
		<img src="../images/Healthcare – 2.png" alt=""/>
		<h4>Cars</h4>
		<p>I think in the future, AR technology can recognize more and more things in real life. Like the sign with the max speed allowed and be able to follow the license plate and calculate its speed. This way it will be easier for the police to monitor the roads. As much as it is annoying to get a fine, it’s so important to get the rulebreakers the consequences. So they stop driving fast, this will save many more lives in the future.</p>
		<img src="../images/Cars - 1.png" alt=""/>
		<h4>Public Transport</h4>
		<p>Usually, the bus or train has a name on the side to indicate its destination. But sometimes you might have missed it. With this QR code on the side, next to every door, you will be able to see its destination, departure time and the next vehicle with its destination as well. With this you’ll be able to always know the information you want to know. In the image I put the code on the lower side, other models or vehicles might be able to put the code higher. So people can’t walk through your code and camera.</p>
		<img src="../images/Public Transport - 1.png" alt=""/>
		<img src="../images/Public Transport - 2.png" alt=""/>
		<h4>Research</h4>
		<img src="../images/ARRESEARCH.png" alt=""/>
			<p>The project I found was a AR one. It detects the sign language of the person in front of the camera. The app combines computer vision with AR to capture specific sign language hand gestures performed in front of the camera and provide a real-time translation in the native language of the user. Developed by Heng Li, Jacky Chen, and Mingfei Huang.</p><p>
This project took an interest on me because I’m myself hearing impaired. Even though I didn’t have to learn sign language, was always a bit interested in how it worked. Now with this app it is even easier to learn and understand! This is great for the sign language community!
</p><a href="https://vrscout.com/projects/prototype-ar-app-translates-sign-language/">Link to project</a></div>
	</article>

	<article id="LEC2">
		<h1>Wearables</h1>
		<div>
			<img src="../images/Wearable 1.png" alt= ""/>
			<img src="../images/Wearable 2.png" alt=""/>
		<p>The concept is an visualisation of an estimation of the users feelings. Through sensor that reads the bpm, step counter and electrode sensor on the head. It computes this information in two different fields of health. Mental and physical. It will give an average number and give a smiley if its above a certain value. A smile for an average above 5 and sad face for lower than 5. This smiley is shown on a LED Matrix.</p>
			<h2>Research</h2>
			<img src="../images/WRRESEARCH.png" alt=""/>
			<p>this is an Arduino project focused on making cyclist more visible in the night. Through the use of a gyroscope, the LED on the user's back can indicate where he or she is going. When said user is going to the left, it will display an arrow to the left on the back of the cyclist. When the user starts braking, it will display red circle to indicate the braking. What I love about this project is the simplicity, yet the effectiveness is through the roof!</p>
			<a href="https://create.arduino.cc/projecthub/eben-kouao/diy-arduino-turn-signal-bike-safety-vest-3f5a17?ref=tag&ref_id=wearables&offset=0">Link to project</a>
		</div>
		
	</article>
	

	<article id="LEC3">
		<h1>Artificial Creatures</h1>
		<div><p>A printer only has a few needs, paper and ink. When a user turns on the printer, it needs to start up a bit to calibrate the inkjet and secure a good connection. You can imagine this as morning routine, like waking up but as a printer. Stretching its inkjet and machinery. Opening the paper tray as a yawn. Ink could work as a hydrant. When the printer runs low on ink, it starts to be de-hydrated. Get headaches and be more tired and asking for more ink. Encouraging the user to refill its ink tank. When the use does refill the ink, it will thank the user and will look satisfied. Also when the paper runs out, it will look a bit confused and ask for more paper. When given the paper, it will look happy again and resume printing again. After finishing a job, it will smile and ask for more! It is that excited! When the user is done printing, it can the turn off the printer and it will go literally go back to sleep. To show the emotion and different stages of the printer, add a small screen to it with a face. That will respond to the different stages I mentioned here above.</p><h2>Research</h2><img src="../images/ACRESEARCH.png" alt=""/><p>Even though it isn’t a artificial creatures, it still makes us more artificial without it. Now people with limb or nerve damage can get walking, jumping , climbing and dancing again! This project is about empowering humans who got into an accident with their legs. At a young age, Hugh Herr, lost his leg in a climbing accident with frostbite. Instead of blaming the human body, he blamed the technology that wasn’t capable of fixing his disability. Thus he went out and worked hard to become a MIT Professor and built himself a pair of bionic leg. I found this project very profound, as someone with a  disability myself (hearing impaired). I found this very inspiring for someone to get up and to fix their own disability through technology.</p>
			
			<a href="https://robohub.org/human-2-0-exoskeletons-and-orthoses/">Link to project</a>
		</div>
		
	</article>
	<article id="LEC4">
		<h1>Playful Interfaces</h1><div>
		<h4>Assignment A:</h4>
		<p>Imagine a big black screen in front of an empty space where people can play Pong with their bodies. To initiate the game, two players needs to be waving towards the screen simultaneously. When the game starts to block will move to one side and that player has to block it by moving the bar. One does so by raising or lowering her or his hand. This hand will be watched by camera attached to a computer, that was machine learned to recognize the hand and its gestures. It will process this data in two dots on a 2D plane. When one hand goes higher, so will the dot on the plane. So the computer knows exactly, according to the position of the dot where the bar needs to be put.</p><img src="../images/PONG.png" alt="" /><img src="../images/PONG 2.png" alt="" />
		<h4>Assignment B:</h4>
		<img src="../images/REACTABLE.png" alt=""/>
		<p>This a table called the Reactable. ROTOR includes dozens of multitouch control panels such as virtual keyboards, polyphonic and monophonic step-sequencers, envelope generators, or 2D panels, which empower to control in real-time, every detail or nuance of the performance. On the other hand, interconnecting less linear and less predictable modules, such as the accelerometer input, feedback, etc., opens a whole universe of complex generative and serendipitous creations. Of course this is interface will take a bit to master. But once the user knows everything he needs, the results are endless! Instead of winning against others, now your winning price is the best music you can make!</p><a href="https://youtu.be/Yv4kN7kDFB4">Link to YouTube video</a>
		<h4>Assignment C:</h4><p>Playful interfaces was a real interesting topic! It’s almost an extension of another course we have, called User Experience. Where we have to design a fun concept where customers are unknowingly endeared to buy  a product. Play interfaces is like that but different. Its goal is to entertain users instead of getting to buy something. 
I wouldn’t be surprised if a lot more interfaces are becoming a lot more playful. Not only does this mean they will be more efficient, but also more meaningful for the user. Making it more effective!
</p></div>
	</article>
	</main>
</body>
</html>
